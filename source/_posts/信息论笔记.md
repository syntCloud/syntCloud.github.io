---
title: 信息论笔记
date: 2024-06-21 01:53:00
tags: [信息论]
categories: 学习
---

还是复习。

<!--more-->

## 第一章 绪论

## 第二章 离散信息的度量

## 第三章 离散信源

## 第四章 连续信息与连续信源

## 第五章 无失真信源编码

## 第六章 离散信道及其容量

## 第七章 有噪信道编码

## 第八章 波形信道

这个例题是关于离散时间无记忆加性噪声信道的输入和输出的互信息、信道容量及其概率分布的求解。具体问题和解答如下：

**习题** 8.3

一个离散时间无记忆加性噪声信道的输入 \( X \) 限制在 \([-2,2]\)，独立于 \( X \) 的噪声 \( Z \) 在 \([-1,1]\) 区间均匀分布，熵为 \( h(Z) \)。信道输出 \( Y \) 的熵为 \( h(Y) \)。

1. 写出信道输入 \( X \) 与输出 \( Y \) 的平均互信息 \( I(X;Y) \) 的表达式。
2. 求信道容量和达到容量时的输出概率分布。
3. 求达到容量时的输入概率分布。

**解**：

1. \( I(X;Y) \) 的表达式： \( I(X;Y) = h(Y) - h(Z) \)。

2. 因为 \( -2 \le X \le 2 \)， \( -1 \le Z \le 1 \)。所以 \( y = x + z \)，噪声熵 \( h(Z) = \log(1 + 1) = \log 2 \)，所以当 \( Y \) 有最大熵时，信道达到容量，此时 \( Y \) 应在 \([-3,3]\) 范围均匀分布，\( Y \) 的分布密度为
\[ p_Y(y) = \begin{cases}
\frac{1}{6} & -3 < y < 3 \\
0 & \text{其他}
\end{cases} \]

    信道容量 \( \max I(X;Y) = \log 2 (3 + 3) - \log 2 = \log 2 3 = 1.585 \) 比特/自由度。

3. 因为 \( y = x + z \)，且 \( x \) 与 \( z \) 相互独立，则 \( y \) 的概率密度可以由 \( x \) 与 \( z \) 的概率密度卷积得到，设 \( p_X(x) \leftrightarrow X(\omega) \), \( p_Z(z) \leftrightarrow Z(\omega) \), \( p_Y(y) \leftrightarrow Y(\omega) \)，其中 \( \leftrightarrow \) 表示傅里叶变换关系，有 \( X(\omega) Z(\omega) = Y(\omega) \)。

\[
Y(\omega) = \int_{-3}^{3} \left(\frac{1}{6}\right) e^{-j\omega y} dy = \frac{\sin(3\omega)}{3\omega}
\]

\[
Z(\omega) = \int_{-1}^{1} \left(\frac{1}{2}\right) e^{-j\omega z} dz = \frac{\sin \omega}{\omega}
\]

\[
X(\omega) = \frac{Y(\omega)}{Z(\omega)} = \frac{1}{3} (2 \cos 2\omega + 1)
\]

做反变换，得

\[
X(\omega) = \frac{1}{3} (2 \cos 2\omega + 1) = \frac{1}{3} \left[ \delta(x-2) + \delta(x+2) + \delta(x) \right]
\]

所以达到容量时，\( X \) 的概率分布是

\[
p(x=0) = p(x=-2) = p(x=2) = \frac{1}{3}
\]

通过傅里叶变换和反变换，我们得到了 \( X \) 的概率分布，这也就是在信道达到容量时输入的概率分布。

**习题** 8.6

设离散时间连续信道的输入与输出分别为 \( X^N = (X_1, \cdots, X_N) \) 和 \( Y^N = (Y_1, \cdots, Y_N) \)，试证明：

1. 信源无记忆时，有
   \[
   I(X^N; Y^N) \geq \sum_{i=1}^N I(X_i; Y_i)
   \]
   当且仅当信道无记忆时等式成立。

2. 信道无记忆时，有
   \[
   I(X^N; Y^N) \leq \sum_{i=1}^N I(X_i; Y_i)
   \]
   当且仅当信源无记忆时等式成立。

**证明**：

1. \[
   \sum_{i=1}^N I(X_i; Y_i) - I(X^N; Y^N) = \sum_{i=1}^N [H(X_i) - H(X_i | Y_i)] - H(X^N) + H(X^N | Y^N)
   \]
   \[
   = H(X^N | Y^N) - \sum_{i=1}^N H(X_i | Y_i) + \sum_{i=1}^N H(X_i) - H(X^N)
   \]
   \[
   = H(X^N | Y^N) - \sum_{i=1}^N H(X_i | Y_i)
   \]
   故：
   \[
   I(X^N; Y^N) \geq \sum_{i=1}^N I(X_i; Y_i)
   \]

2. \[
   I(X^N; Y^N) - \sum_{i=1}^N I(X_i; Y_i) = H(Y^N) - H(Y^N | X^N) - \sum_{i=1}^N [H(Y_i) - H(Y_i | X_i)]
   \]
   \[
   = H(Y^N) - \sum_{i=1}^N H(Y_i) + \sum_{i=1}^N H(Y_i | X_i) - H(Y^N | X^N) \leq 0
   \]

综上所述，当信源和信道无记忆时，这两个等式分别成立。
