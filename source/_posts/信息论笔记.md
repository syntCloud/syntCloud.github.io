---
title: 信息论笔记
date: 2024-06-21 01:53:00
tags: [信息论]
categories: 学习
---

还是复习。

<!--more-->

## 第一章 绪论

不提

## 第二章 离散信息的度量

**习题** 2.6
在某地篮球联赛的每个赛季中，最终只有 A、B 两队进入决赛争夺冠军。决赛采用 7 场 4 胜制，首先赢得 4 场胜利的球队获得冠军，并结束比赛。把产生冠军的事件 x 用 A、B 两队各场次比赛结果表示，作为信源 X 产生的随机事件，例如：AAAA 表示事件“A 队前 4 场获得冠军”；ABBAAA 表示事件“ A 队在第 1、4、5、6 场取胜获得冠军（而 B 队在第 2、3 场取胜）”……假设两队在每场比赛中的取胜机会均等，每场比赛只有“A 胜”或“B 胜”两种结果，并且各场比赛的结果是彼此独立的。

（1）求信源的熵 H(X)。
| 事件 | 数目 | 单事件的概率 | 概率的和 |
| :--- | :--- | :----------- | :------- |
| 赛4场获冠军（AAAA） | 1 | 1/16 | 1/16 |
| 赛5场获冠军（前 4 场 B 胜 1 场） | 4 | 1/32 | 1/8 |
| 赛6场获冠军（前 5 场 B 胜 2 场） | 10 | 1/64 | 5/32 |
| 赛7场获冠军（前 6 场 B 胜 3 场） | 20 | 1/128 | 5/32 |
| **总概率** |   |   | 1/2 |

$H(X) = 2 \times \left[ (1/16) \log_2 16 + 4 \times (1/32) \log_2 32 + 10 \times (1/64) \log_2 64 + 20 \times (1/128) \log_2 128 \right] = 186/32 = 5.8125 $ 比特/符号

（2）求事件“两队打满 7 场”所提供的信息量。
“两队打满7场”事件数为 40，所求概率为 $40 \times (1/128) = 5/16$ ，事件“两队打满7场”所提供的信息量为
$$I_1 = -\log_2 (5/16) = 1.6781 \text{ bit}$$
（3）列出 A 队前 3 场都失利的所有情形，求“A 队前 3 场都失利”所提供的信息量。
如表所示：
| 事件 | 概率 | 结果 |
| :--- | :--- | :--- |
| BBBB | 1/16 | B 夺冠 |
| BBBAB | 1/32 | B 夺冠 |
| BBBAAB | 1/64 | B 夺冠 |
| BBBAAAB | 1/128 | B 夺冠 |
| BBBAAAA | 1/128 | A 夺冠 |

$$I_2 = \log_2 8 = 3 \text{ bit}$$
（4）求事件“A 队在前 3 场获胜的条件下又取得冠军”所提供的信息量。
A队在前3场失利的条件下又取得冠军的条件概率为
$$\frac{1/128}{1/16 + 1/32 + 1/64 + 2 \times (1/128)}  = \frac{1}{16}$$
“A队在前3场失利的条件下又取得冠军”的信息量为
$$I_3 = \log_2 16 = 4 \text{ bit}$$

**习题** 2.7
已知随机变量 $X, Y$ 的联合概率分布为 $P_{XY}(a_k, b_j)$ ，满足：
$P_X(a_1) = 1/2$, $P_X(a_2) = P_X(a_3) = 1/4$,
$P_Y(b_1) = 2/3$, $P_Y(b_2) = P_Y(b_3) = 1/6$。试求能使 $H(XY)$ 取最大值的 $XY$ 的联合概率分布。

**习题** 2.16
两随机变量 $X, Y$ ，联合概率 $p(x,y)$ 如下：

|   | y=0 | y=1 |
|---|-----|-----|
| x=0 | 1/8 | 3/8 |
| x=1 | 3/8 | 1/8 |

$Z = X \cdot Y$（一般乘积），试计算：

(1) $H(X), H(Y), H(Z), H(XZ), H(YZ), H(XYZ)$;
(2) $H(X|Y), H(Y|X), H(X|Z), H(Z|X), H(Y|Z), H(Z|Y), H(Z|YZ), H(Y|XZ), H(Z|XY)$;
(3) $I(X;Y), I(X;Z), I(Y;Z), I(X;Y|Z), I(Y;Z|X), I(X;Z|Y)$。

## 第三章 离散信源

**习题** 3.9
设 $x_1, x_2, \cdots, x_{n-1}, x_n, \cdots$ 为平稳序列（未必是马氏链），那么下面的论断哪些是正确的？对正确的进行证明，对错误的举出反例。（提示：下面论断至少有一个是错的）
(1) $H(X_n | X_0) = H(X_{n-1} | X_0)$;
(2) $H(X_n | X_0) \geq H(X_{n-1} | X_0)$;
(3) $H(X_n | X_1, X_2, \cdots, X_{n-1})$ 是 $n$ 的增函数;
(4) $H(X_n | X_1, X_2, \cdots, X_{n-1}, X_{n+1}, \cdots, X_{2n})$ 是 $n$ 的非增函数。

**习题** 3.10
一个 2 状态马氏链的转移概率矩阵为
$$P = \begin{pmatrix} 3/4 & 1/4 \\ 1/4 & 3/4 \end{pmatrix}$$
并假定初始状态概率矢量为 $p^{(0)} = (1 \ 0)$；求
(1) $P^n$ 和 $p^{(n)}$, $n = 1, 2, 3$；
(2) $P^n$ 和 $p^{(n)}$ 的一般形式。

**习题** 3.15
3.15 黑白气象传真图的消息只有黑色和白色两种，即信源 $X=\{\text{黑}, \text{白}\}$ ；设黑色出现的概率为 $p(\text{黑}) = 0.3$ ，白色的出现概率 $p(\text{白}) = 0.7$ ：

(1) 假设图上黑白消息出现前后没有关联，求熵 $H(X)$ ；
(2) 假设消息前后有关联，其依赖关系为 $P(\text{白}|\text{白}) = 0.9$, $P(\text{黑}|\text{白}) = 0.1$, $P(\text{白}|\text{黑}) = 0.2$, $P(\text{黑}|\text{黑}) = 0.8$，求此一阶马氏链的熵率 $H_2$；
(3) 分别求上述两种信源的剩余度，并比较 $H(X)$ 和 $H_2$ 的大小，并说明其物理意义。

**习题** 3.21

## 第四章 连续信息与连续信源

**习题** 4.1

**习题** 4.13

**习题** 4.14

**习题** 4.20

## 第五章 无失真信源编码

**习题** 5.1

**习题** 5.5

**习题** 5.7

**习题** 5.8

## 第六章 离散信道及其容量

**习题** 6.6

**习题** 6.11

**习题** 6.12

**习题** 6.14

## 第七章 有噪信道编码

**习题** 7.3

**习题** 7.4

## 第八章 波形信道

这个例题是关于离散时间无记忆加性噪声信道的输入和输出的互信息、信道容量及其概率分布的求解。具体问题和解答如下：

**习题** 8.3

一个离散时间无记忆加性噪声信道的输入 \( X \) 限制在 \([-2,2]\)，独立于 \( X \) 的噪声 \( Z \) 在 \([-1,1]\) 区间均匀分布，熵为 \( h(Z) \)。信道输出 \( Y \) 的熵为 \( h(Y) \)。

1. 写出信道输入 \( X \) 与输出 \( Y \) 的平均互信息 \( I(X;Y) \) 的表达式。
2. 求信道容量和达到容量时的输出概率分布。
3. 求达到容量时的输入概率分布。

**解**：

1. \( I(X;Y) \) 的表达式： \( I(X;Y) = h(Y) - h(Z) \)。

2. 因为 \( -2 \le X \le 2 \)， \( -1 \le Z \le 1 \)。所以 \( y = x + z \)，噪声熵 \( h(Z) = \log(1 + 1) = \log 2 \)，所以当 \( Y \) 有最大熵时，信道达到容量，此时 \( Y \) 应在 \([-3,3]\) 范围均匀分布，\( Y \) 的分布密度为
\[ p_Y(y) = \begin{cases}
\frac{1}{6} & -3 < y < 3 \\
0 & \text{其他}
\end{cases} \]

    信道容量 \( \max I(X;Y) = \log 2 (3 + 3) - \log 2 = \log 2 3 = 1.585 \) 比特/自由度。

3. 因为 \( y = x + z \)，且 \( x \) 与 \( z \) 相互独立，则 \( y \) 的概率密度可以由 \( x \) 与 \( z \) 的概率密度卷积得到，设 \( p_X(x) \leftrightarrow X(\omega) \), \( p_Z(z) \leftrightarrow Z(\omega) \), \( p_Y(y) \leftrightarrow Y(\omega) \)，其中 \( \leftrightarrow \) 表示傅里叶变换关系，有 \( X(\omega) Z(\omega) = Y(\omega) \)。

\[
Y(\omega) = \int_{-3}^{3} \left(\frac{1}{6}\right) e^{-j\omega y} dy = \frac{\sin(3\omega)}{3\omega}
\]

\[
Z(\omega) = \int_{-1}^{1} \left(\frac{1}{2}\right) e^{-j\omega z} dz = \frac{\sin \omega}{\omega}
\]

\[
X(\omega) = \frac{Y(\omega)}{Z(\omega)} = \frac{1}{3} (2 \cos 2\omega + 1)
\]

做反变换，得

\[
X(\omega) = \frac{1}{3} (2 \cos 2\omega + 1) = \frac{1}{3} \left[ \delta(x-2) + \delta(x+2) + \delta(x) \right]
\]

所以达到容量时，\( X \) 的概率分布是

\[
p(x=0) = p(x=-2) = p(x=2) = \frac{1}{3}
\]

通过傅里叶变换和反变换，我们得到了 \( X \) 的概率分布，这也就是在信道达到容量时输入的概率分布。

**习题** 8.6

设离散时间连续信道的输入与输出分别为 \( X^N = (X_1, \cdots, X_N) \) 和 \( Y^N = (Y_1, \cdots, Y_N) \)，试证明：

1. 信源无记忆时，有
   \[
   I(X^N; Y^N) \geq \sum_{i=1}^N I(X_i; Y_i)
   \]
   当且仅当信道无记忆时等式成立。

2. 信道无记忆时，有
   \[
   I(X^N; Y^N) \leq \sum_{i=1}^N I(X_i; Y_i)
   \]
   当且仅当信源无记忆时等式成立。

**证明**：

1. \[
   \sum_{i=1}^N I(X_i; Y_i) - I(X^N; Y^N) = \sum_{i=1}^N [H(X_i) - H(X_i | Y_i)] - H(X^N) + H(X^N | Y^N)
   \]
   \[
   = H(X^N | Y^N) - \sum_{i=1}^N H(X_i | Y_i) + \sum_{i=1}^N H(X_i) - H(X^N)
   \]
   \[
   = H(X^N | Y^N) - \sum_{i=1}^N H(X_i | Y_i)
   \]
   故：
   \[
   I(X^N; Y^N) \geq \sum_{i=1}^N I(X_i; Y_i)
   \]

2. \[
   I(X^N; Y^N) - \sum_{i=1}^N I(X_i; Y_i) = H(Y^N) - H(Y^N | X^N) - \sum_{i=1}^N [H(Y_i) - H(Y_i | X_i)]
   \]
   \[
   = H(Y^N) - \sum_{i=1}^N H(Y_i) + \sum_{i=1}^N H(Y_i | X_i) - H(Y^N | X^N) \leq 0
   \]

综上所述，当信源和信道无记忆时，这两个等式分别成立。

## 第九章 信息率失真函数

不用考，不学啦！
